{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - Inicialmente iremos preparar o ambiente, definindo o diretório onde nosso código será desenvolvido. Para este diretório iremos copiar o arquivo nomes_aleatorios.txt.\n",
    "Após, em nosso script Python, devemos importar as bibliotecas necessárias:\n",
    "    \n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark import SparkContext, SQLContext\n",
    "    spark = SparkSession \\\n",
    "\n",
    "                .builder \\\n",
    "                .master(\"local[*]\")\\\n",
    "                .appName(\"Exercicio Intro\") \\\n",
    "                .getOrCreate()\n",
    "\n",
    "Nesta etapa, adicione código para ler o arquivo nomes_aleatorios.txt através do comando spark.read.csv. Carregue-o para dentro de um dataframe chamado df_nomes e, por fim, liste algumas linhas através do método show. Exemplo: df_nomes.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|           value|\n",
      "+----------------+\n",
      "|  Frances Bennet|\n",
      "|   Jamie Russell|\n",
      "|  Edward Kistler|\n",
      "|   Sheila Maurer|\n",
      "|Donald Golightly|\n",
      "+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Criando a Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Exercicio Intro\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Lendo o arquivo nomes_aleatorios.txt e carregando-o em um DataFrame\n",
    "caminho_arquivo = \"../Gerar_Massa_de_Dados/nomes_aleatorios.txt\"\n",
    "df_nomes = spark.read.text(caminho_arquivo)\n",
    "\n",
    "# Mostrando as primeiras 5 linhas do DataFrame\n",
    "df_nomes.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - No Python, é possível acessar uma coluna de um objeto dataframe pelo atributo (por exemplo df_nomes.nome) ou por índice (df_nomes['nome']). Enquanto a primeira forma é conveniente para a exploração de dados interativos, você deve usar o formato de índice, pois caso algum nome de coluna não esteja de acordo seu código irá falhar.\n",
    "\n",
    "Como não informamos no momento da leitura do arquivo, o Spark não identificou o Schema por padrão e definiu todas as colunas como string. Para ver o Schema, use o método df_nomes.printSchema().\n",
    "\n",
    "Nesta etapa, será necessário adicionar código para renomear a coluna para Nomes, imprimir o esquema e mostrar 10 linhas do dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 - Ao dataframe (df_nomes), adicione nova coluna chamada Escolaridade e atribua para cada linha um dos três valores de forma aleatória: Fundamental, Medio ou Superior.\n",
    "\n",
    "Para esta etapa, evite usar funções de iteração, como por exemplo: for, while, entre outras. Dê preferência aos métodos oferecidos para próprio Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 - Ao dataframe (df_nomes), adicione nova coluna chamada Pais e atribua para cada linha o nome de um dos 13 países da América do Sul, de forma aleatória.\n",
    "\n",
    "Para esta etapa, evite usar funções de iteração, como por exemplo: for, while, entre outras. Dê preferência aos métodos oferecidos para próprio Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+---------+\n",
      "|            Nomes|Escolaridade|     Pais|\n",
      "+-----------------+------------+---------+\n",
      "|   Frances Bennet|    Superior|  Equador|\n",
      "|    Jamie Russell| Fundamental|  Bolívia|\n",
      "|   Edward Kistler|    Superior|    Chile|\n",
      "|    Sheila Maurer|       Médio|   Brasil|\n",
      "| Donald Golightly|       Médio| Paraguai|\n",
      "|       David Gray|       Médio|    Chile|\n",
      "|      Joy Bennett|       Médio| Paraguai|\n",
      "|      Paul Kriese|       Médio|  Equador|\n",
      "|Berniece Ornellas|       Médio|     Peru|\n",
      "|    Brian Farrell|    Superior|Argentina|\n",
      "+-----------------+------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import rand, when\n",
    "from random import randint\n",
    "\n",
    "# Criando a Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Exercicio Intro\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 1- Lendo o arquivo nomes_aleatorios.txt e carregando-o em um DataFrame\n",
    "caminho_arquivo = \"../Gerar_Massa_de_Dados/nomes_aleatorios.txt\"\n",
    "df_nomes = spark.read.text(caminho_arquivo)\n",
    "\n",
    "# 2- Renomeando a coluna para 'Nomes'\n",
    "df_nomes = df_nomes.withColumnRenamed(\"value\", \"Nomes\")\n",
    "\n",
    "# 3 - Adicionando a nova coluna 'Escolaridade' com valores aleatórios\n",
    "df_nomes = df_nomes.withColumn('Escolaridade', when(rand() < 0.33, 'Fundamental')\n",
    "                                          .when(rand() < 0.66, 'Médio')\n",
    "                                          .otherwise('Superior'))\n",
    "\n",
    "# 4 - Lista de países da América do Sul\n",
    "paises_am_sul = [\"Brasil\", \"Argentina\", \"Chile\", \"Colômbia\", \"Equador\", \"Paraguai\", \"Uruguai\", \"Bolívia\", \"Peru\", \"Venezuela\", \"Suriname\", \"Guiana\", \"Guiana Francesa\"]\n",
    "                       \n",
    "# Adicionando a nova coluna com valores aleatórios\n",
    "df_nomes = df_nomes.withColumn(\"Pais\", \n",
    "    when(rand() < 0.077, paises_am_sul[0])\n",
    "    .when(rand() < 0.154, paises_am_sul[1])\n",
    "    .when(rand() < 0.231, paises_am_sul[2])\n",
    "    .when(rand() < 0.308, paises_am_sul[3])\n",
    "    .when(rand() < 0.385, paises_am_sul[4])\n",
    "    .when(rand() < 0.462, paises_am_sul[5])\n",
    "    .when(rand() < 0.539, paises_am_sul[6])\n",
    "    .when(rand() < 0.616, paises_am_sul[7])\n",
    "    .when(rand() < 0.693, paises_am_sul[8])\n",
    "    .when(rand() < 0.770, paises_am_sul[9])\n",
    "    .when(rand() < 0.847, paises_am_sul[10])\n",
    "    .when(rand() < 0.924, paises_am_sul[11])\n",
    "    .otherwise(paises_am_sul[12]))\n",
    "\n",
    "# Mostrando 10 linhas do DataFrame após a adição da nova coluna\n",
    "df_nomes.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 - Ao dataframe (df_nomes), adicione nova coluna chamada AnoNascimento e atribua para cada linha um valor de ano entre 1945 e 2010, de forma aleatória. \n",
    "\n",
    "\n",
    "Para esta etapa, evite usar funções de iteração, como por exemplo: for, while, entre outras. Dê preferência aos métodos oferecidos para próprio Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+---------+-------------+\n",
      "|Nomes            |Escolaridade|Pais     |AnoNascimento|\n",
      "+-----------------+------------+---------+-------------+\n",
      "|Frances Bennet   |Médio       |Colômbia |1995         |\n",
      "|Jamie Russell    |Médio       |Colômbia |1961         |\n",
      "|Edward Kistler   |Fundamental |Argentina|1976         |\n",
      "|Sheila Maurer    |Médio       |Bolívia  |1961         |\n",
      "|Donald Golightly |Médio       |Colômbia |1970         |\n",
      "|David Gray       |Fundamental |Equador  |1948         |\n",
      "|Joy Bennett      |Fundamental |Chile    |2003         |\n",
      "|Paul Kriese      |Médio       |Paraguai |1968         |\n",
      "|Berniece Ornellas|Superior    |Chile    |1994         |\n",
      "|Brian Farrell    |Médio       |Equador  |1974         |\n",
      "+-----------------+------------+---------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import rand, lit\n",
    "\n",
    "# Criando a Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Exercicio Intro\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 1- Lendo o arquivo nomes_aleatorios.txt e carregando-o em um DataFrame\n",
    "caminho_arquivo = \"../Gerar_Massa_de_Dados/nomes_aleatorios.txt\"\n",
    "df_nomes = spark.read.text(caminho_arquivo)\n",
    "\n",
    "# 2- Renomeando a coluna para 'Nomes'\n",
    "df_nomes = df_nomes.withColumnRenamed(\"value\", \"Nomes\")\n",
    "\n",
    "# 3 - Adicionando a nova coluna 'Escolaridade' com valores aleatórios\n",
    "df_nomes = df_nomes.withColumn('Escolaridade', when(rand() < 0.33, 'Fundamental')\n",
    "                                          .when(rand() < 0.66, 'Médio')\n",
    "                                          .otherwise('Superior'))\n",
    "\n",
    "# 4 - Lista de países da América do Sul\n",
    "paises_am_sul = [\"Brasil\", \"Argentina\", \"Chile\", \"Colômbia\", \"Equador\", \"Paraguai\", \"Uruguai\", \"Bolívia\", \"Peru\", \"Venezuela\", \"Suriname\", \"Guiana\", \"Guiana Francesa\"]\n",
    "                       \n",
    "# Adicionando a nova coluna com valores aleatórios\n",
    "df_nomes = df_nomes.withColumn(\"Pais\", \n",
    "    when(rand() < 0.077, paises_am_sul[0])\n",
    "    .when(rand() < 0.154, paises_am_sul[1])\n",
    "    .when(rand() < 0.231, paises_am_sul[2])\n",
    "    .when(rand() < 0.308, paises_am_sul[3])\n",
    "    .when(rand() < 0.385, paises_am_sul[4])\n",
    "    .when(rand() < 0.462, paises_am_sul[5])\n",
    "    .when(rand() < 0.539, paises_am_sul[6])\n",
    "    .when(rand() < 0.616, paises_am_sul[7])\n",
    "    .when(rand() < 0.693, paises_am_sul[8])\n",
    "    .when(rand() < 0.770, paises_am_sul[9])\n",
    "    .when(rand() < 0.847, paises_am_sul[10])\n",
    "    .when(rand() < 0.924, paises_am_sul[11])\n",
    "    .otherwise(paises_am_sul[12]))\n",
    "\n",
    "# 5 - Adicionando a nova coluna 'AnoNascimento' com valores aleatórios entre 1945 e 2010\n",
    "df_nomes = df_nomes.withColumn(\"AnoNascimento\", (lit(1945) + (rand() * (2010 - 1945))).cast(\"int\"))\n",
    "\n",
    "\n",
    "# Mostrando 10 linhas do DataFrame após a adição da nova coluna\n",
    "df_nomes.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 - Usando o método select do dataframe (df_nomes), selecione as pessoas que nasceram neste século. Armazene o resultado em outro dataframe chamado df_select e mostre 10 nomes deste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+---------+\n",
      "|Nomes           |AnoNascimento|Pais     |\n",
      "+----------------+-------------+---------+\n",
      "|Tracy Herring   |2004         |Chile    |\n",
      "|Page Marthe     |2008         |Argentina|\n",
      "|Gabriel Colyer  |2003         |Argentina|\n",
      "|Jerry Chynoweth |2002         |Chile    |\n",
      "|Michael Agnew   |2008         |Chile    |\n",
      "|George Miller   |2005         |Colômbia |\n",
      "|Juliet Liles    |2007         |Colômbia |\n",
      "|Maurice Blizzard|2001         |Brasil   |\n",
      "|Shelia Ceja     |2004         |Chile    |\n",
      "|Joyce Bennett   |2007         |Equador  |\n",
      "+----------------+-------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import rand, lit\n",
    "\n",
    "# Criando a Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Exercicio Intro\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 1- Lendo o arquivo nomes_aleatorios.txt e carregando-o em um DataFrame\n",
    "caminho_arquivo = \"../Gerar_Massa_de_Dados/nomes_aleatorios.txt\"\n",
    "df_nomes = spark.read.text(caminho_arquivo)\n",
    "\n",
    "# 2- Renomeando a coluna para 'Nomes'\n",
    "df_nomes = df_nomes.withColumnRenamed(\"value\", \"Nomes\")\n",
    "\n",
    "# 3 - Adicionando a nova coluna 'Escolaridade' com valores aleatórios\n",
    "df_nomes = df_nomes.withColumn('Escolaridade', when(rand() < 0.33, 'Fundamental')\n",
    "                                          .when(rand() < 0.66, 'Médio')\n",
    "                                          .otherwise('Superior'))\n",
    "\n",
    "# 4 - Lista de países da América do Sul\n",
    "paises_am_sul = [\"Brasil\", \"Argentina\", \"Chile\", \"Colômbia\", \"Equador\", \"Paraguai\", \"Uruguai\", \"Bolívia\", \"Peru\", \"Venezuela\", \"Suriname\", \"Guiana\", \"Guiana Francesa\"]\n",
    "                       \n",
    "# Adicionando a nova coluna com valores aleatórios\n",
    "df_nomes = df_nomes.withColumn(\"Pais\", \n",
    "    when(rand() < 0.077, paises_am_sul[0])\n",
    "    .when(rand() < 0.154, paises_am_sul[1])\n",
    "    .when(rand() < 0.231, paises_am_sul[2])\n",
    "    .when(rand() < 0.308, paises_am_sul[3])\n",
    "    .when(rand() < 0.385, paises_am_sul[4])\n",
    "    .when(rand() < 0.462, paises_am_sul[5])\n",
    "    .when(rand() < 0.539, paises_am_sul[6])\n",
    "    .when(rand() < 0.616, paises_am_sul[7])\n",
    "    .when(rand() < 0.693, paises_am_sul[8])\n",
    "    .when(rand() < 0.770, paises_am_sul[9])\n",
    "    .when(rand() < 0.847, paises_am_sul[10])\n",
    "    .when(rand() < 0.924, paises_am_sul[11])\n",
    "    .otherwise(paises_am_sul[12]))\n",
    "\n",
    "# 5 - Adicionando a nova coluna 'AnoNascimento' com valores aleatórios entre 1945 e 2010\n",
    "df_nomes = df_nomes.withColumn(\"AnoNascimento\", (lit(1945) + (rand() * (2010 - 1945))).cast(\"int\"))\n",
    "\n",
    "# 6 - Filtrando as pessoas que nasceram neste século (após o ano 2000)\n",
    "df_select = df_nomes.filter(df_nomes.AnoNascimento >= 2000)\n",
    "\n",
    "# Selecionando apenas as colunas desejadas\n",
    "df_select = df_select.select(\"Nomes\", \"AnoNascimento\", \"Pais\")\n",
    "\n",
    "# Mostrando 10 linhas do DataFrame df_select\n",
    "df_select.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando Spark SQL repita o processo da Pergunta 6. Lembre-se que, para trabalharmos com SparkSQL, precisamos registrar uma tabela temporária e depois executar o comando SQL. Abaixo um exemplo de como executar comandos SQL com SparkSQL:\n",
    "\n",
    "\n",
    "\n",
    "df_nomes.createOrReplaceTempView (\"pessoas\")\n",
    "\n",
    "spark.sql(\"select * from pessoas\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+---------+\n",
      "|Nomes          |AnoNascimento|Pais     |\n",
      "+---------------+-------------+---------+\n",
      "|Frances Bennet |2004         |Paraguai |\n",
      "|Edward Kistler |2003         |Chile    |\n",
      "|Sheila Maurer  |2004         |Equador  |\n",
      "|Ernest Hulet   |2004         |Uruguai  |\n",
      "|Wilfredo Grant |2008         |Uruguai  |\n",
      "|Rosie Lovelady |2004         |Uruguai  |\n",
      "|Lynne Dustman  |2001         |Argentina|\n",
      "|Jason Martin   |2000         |Colômbia |\n",
      "|Bernard Holmes |2009         |Chile    |\n",
      "|Elizabeth Crowe|2007         |Chile    |\n",
      "+---------------+-------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import rand, lit\n",
    "\n",
    "# Criando a Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Exercicio Intro\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 1- Lendo o arquivo nomes_aleatorios.txt e carregando-o em um DataFrame\n",
    "caminho_arquivo = \"../Gerar_Massa_de_Dados/nomes_aleatorios.txt\"\n",
    "df_nomes = spark.read.text(caminho_arquivo)\n",
    "\n",
    "# 2- Renomeando a coluna para 'Nomes'\n",
    "df_nomes = df_nomes.withColumnRenamed(\"value\", \"Nomes\")\n",
    "\n",
    "# 3 - Adicionando a nova coluna 'Escolaridade' com valores aleatórios\n",
    "df_nomes = df_nomes.withColumn('Escolaridade', when(rand() < 0.33, 'Fundamental')\n",
    "                                          .when(rand() < 0.66, 'Médio')\n",
    "                                          .otherwise('Superior'))\n",
    "\n",
    "# 4 - Lista de países da América do Sul\n",
    "paises_am_sul = [\"Brasil\", \"Argentina\", \"Chile\", \"Colômbia\", \"Equador\", \"Paraguai\", \"Uruguai\", \"Bolívia\", \"Peru\", \"Venezuela\", \"Suriname\", \"Guiana\", \"Guiana Francesa\"]\n",
    "                       \n",
    "# Adicionando a nova coluna com valores aleatórios\n",
    "df_nomes = df_nomes.withColumn(\"Pais\", \n",
    "    when(rand() < 0.077, paises_am_sul[0])\n",
    "    .when(rand() < 0.154, paises_am_sul[1])\n",
    "    .when(rand() < 0.231, paises_am_sul[2])\n",
    "    .when(rand() < 0.308, paises_am_sul[3])\n",
    "    .when(rand() < 0.385, paises_am_sul[4])\n",
    "    .when(rand() < 0.462, paises_am_sul[5])\n",
    "    .when(rand() < 0.539, paises_am_sul[6])\n",
    "    .when(rand() < 0.616, paises_am_sul[7])\n",
    "    .when(rand() < 0.693, paises_am_sul[8])\n",
    "    .when(rand() < 0.770, paises_am_sul[9])\n",
    "    .when(rand() < 0.847, paises_am_sul[10])\n",
    "    .when(rand() < 0.924, paises_am_sul[11])\n",
    "    .otherwise(paises_am_sul[12]))\n",
    "\n",
    "# 5 - Adicionando a nova coluna 'AnoNascimento' com valores aleatórios entre 1945 e 2010\n",
    "df_nomes = df_nomes.withColumn(\"AnoNascimento\", (lit(1945) + (rand() * (2010 - 1945))).cast(\"int\"))\n",
    "\n",
    "# 6 - Filtrando as pessoas que nasceram neste século (após o ano 2000)\n",
    "# df_select = df_nomes.filter(df_nomes.AnoNascimento >= 2000)\n",
    "\n",
    "# 7 - Registrar o DataFrame como uma tabela temporária\n",
    "df_nomes.createOrReplaceTempView(\"pessoas\")\n",
    "\n",
    "# Executar consultas SQL usando Spark SQL\n",
    "consulta_sql = \"\"\"\n",
    "    SELECT Nomes, AnoNascimento, Pais\n",
    "    FROM pessoas\n",
    "    WHERE AnoNascimento >= 2000\n",
    "\"\"\"\n",
    "\n",
    "# Criar um novo DataFrame com os resultados da consulta SQL\n",
    "df_select = spark.sql(consulta_sql)\n",
    "\n",
    "# Mostrar 10 linhas do DataFrame df_select\n",
    "df_select.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 - Usando o método select do Dataframe df_nomes, Conte o número de pessoas que são da geração Millennials (nascidos entre 1980 e 1994) no Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de pessoas da geração Millennials: 2309295\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import rand, lit\n",
    "\n",
    "\n",
    "# Criando a Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Exercicio Intro\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 1- Lendo o arquivo nomes_aleatorios.txt e carregando-o em um DataFrame\n",
    "caminho_arquivo = \"../Gerar_Massa_de_Dados/nomes_aleatorios.txt\"\n",
    "df_nomes = spark.read.text(caminho_arquivo)\n",
    "\n",
    "# 2- Renomeando a coluna para 'Nomes'\n",
    "df_nomes = df_nomes.withColumnRenamed(\"value\", \"Nomes\")\n",
    "\n",
    "# 3 - Adicionando a nova coluna 'Escolaridade' com valores aleatórios\n",
    "df_nomes = df_nomes.withColumn('Escolaridade', when(rand() < 0.33, 'Fundamental')\n",
    "                                          .when(rand() < 0.66, 'Médio')\n",
    "                                          .otherwise('Superior'))\n",
    "\n",
    "# 4 - Lista de países da América do Sul\n",
    "paises_am_sul = [\"Brasil\", \"Argentina\", \"Chile\", \"Colômbia\", \"Equador\", \"Paraguai\", \"Uruguai\", \"Bolívia\", \"Peru\", \"Venezuela\", \"Suriname\", \"Guiana\", \"Guiana Francesa\"]\n",
    "                       \n",
    "# Adicionando a nova coluna com valores aleatórios\n",
    "df_nomes = df_nomes.withColumn(\"Pais\", \n",
    "    when(rand() < 0.077, paises_am_sul[0])\n",
    "    .when(rand() < 0.154, paises_am_sul[1])\n",
    "    .when(rand() < 0.231, paises_am_sul[2])\n",
    "    .when(rand() < 0.308, paises_am_sul[3])\n",
    "    .when(rand() < 0.385, paises_am_sul[4])\n",
    "    .when(rand() < 0.462, paises_am_sul[5])\n",
    "    .when(rand() < 0.539, paises_am_sul[6])\n",
    "    .when(rand() < 0.616, paises_am_sul[7])\n",
    "    .when(rand() < 0.693, paises_am_sul[8])\n",
    "    .when(rand() < 0.770, paises_am_sul[9])\n",
    "    .when(rand() < 0.847, paises_am_sul[10])\n",
    "    .when(rand() < 0.924, paises_am_sul[11])\n",
    "    .otherwise(paises_am_sul[12]))\n",
    "\n",
    "# 5 - Adicionando a nova coluna 'AnoNascimento' com valores aleatórios entre 1945 e 2010\n",
    "df_nomes = df_nomes.withColumn(\"AnoNascimento\", (lit(1945) + (rand() * (2010 - 1945))).cast(\"int\"))\n",
    "\n",
    "# 6 - Filtrando as pessoas que nasceram neste século (após o ano 2000)\n",
    "# df_select = df_nomes.filter(df_nomes.AnoNascimento >= 2000)\n",
    "\n",
    "# 7 - Registrar o DataFrame como uma tabela temporária\n",
    "# df_nomes.createOrReplaceTempView(\"pessoas\")\n",
    "\n",
    "# Executar consultas SQL usando Spark SQL\n",
    "# consulta_sql = \"\"\"\n",
    "#    SELECT Nomes, AnoNascimento, Pais\n",
    "#    FROM pessoas\n",
    "#    WHERE AnoNascimento >= 2000\n",
    "#\"\"\"\n",
    "\n",
    "# Criar um novo DataFrame com os resultados da consulta SQL\n",
    "# df_select = spark.sql(consulta_sql)\n",
    "\n",
    "# 8 - Selecionar as pessoas da geração Millennials\n",
    "df_millennials = df_nomes.select(\"Nomes\", \"AnoNascimento\") \\\n",
    "                         .filter((df_nomes.AnoNascimento >= 1980) & (df_nomes.AnoNascimento <= 1994))\n",
    "\n",
    "# Contar o número de pessoas da geração Millennials\n",
    "count_millennials = df_millennials.count()\n",
    "\n",
    "# Mostrar o resultado\n",
    "print(f\"Número de pessoas da geração Millennials: {count_millennials}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|NumeroMillennials|\n",
      "+-----------------+\n",
      "|          2307604|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import rand, lit\n",
    "\n",
    "\n",
    "# Criando a Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Exercicio Intro\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 1- Lendo o arquivo nomes_aleatorios.txt e carregando-o em um DataFrame\n",
    "caminho_arquivo = \"../Gerar_Massa_de_Dados/nomes_aleatorios.txt\"\n",
    "df_nomes = spark.read.text(caminho_arquivo)\n",
    "#--------------------------------------------------------------------------------------\n",
    "# 2- Renomeando a coluna para 'Nomes'\n",
    "df_nomes = df_nomes.withColumnRenamed(\"value\", \"Nomes\")\n",
    "#--------------------------------------------------------------------------------------\n",
    "# 3 - Adicionando a nova coluna 'Escolaridade' com valores aleatórios\n",
    "df_nomes = df_nomes.withColumn('Escolaridade', when(rand() < 0.33, 'Fundamental')\n",
    "                                          .when(rand() < 0.66, 'Médio')\n",
    "                                          .otherwise('Superior'))\n",
    "#--------------------------------------------------------------------------------------\n",
    "# 4 - Lista de países da América do Sul\n",
    "paises_am_sul = [\"Brasil\", \"Argentina\", \"Chile\", \"Colômbia\", \"Equador\", \"Paraguai\", \"Uruguai\", \"Bolívia\", \"Peru\", \"Venezuela\", \"Suriname\", \"Guiana\", \"Guiana Francesa\"]\n",
    "                       \n",
    "# Adicionando a nova coluna com valores aleatórios\n",
    "df_nomes = df_nomes.withColumn(\"Pais\", \n",
    "    when(rand() < 0.077, paises_am_sul[0])\n",
    "    .when(rand() < 0.154, paises_am_sul[1])\n",
    "    .when(rand() < 0.231, paises_am_sul[2])\n",
    "    .when(rand() < 0.308, paises_am_sul[3])\n",
    "    .when(rand() < 0.385, paises_am_sul[4])\n",
    "    .when(rand() < 0.462, paises_am_sul[5])\n",
    "    .when(rand() < 0.539, paises_am_sul[6])\n",
    "    .when(rand() < 0.616, paises_am_sul[7])\n",
    "    .when(rand() < 0.693, paises_am_sul[8])\n",
    "    .when(rand() < 0.770, paises_am_sul[9])\n",
    "    .when(rand() < 0.847, paises_am_sul[10])\n",
    "    .when(rand() < 0.924, paises_am_sul[11])\n",
    "    .otherwise(paises_am_sul[12]))\n",
    "#--------------------------------------------------------------------------------------\n",
    "# 5 - Adicionando a nova coluna 'AnoNascimento' com valores aleatórios entre 1945 e 2010\n",
    "df_nomes = df_nomes.withColumn(\"AnoNascimento\", (lit(1945) + (rand() * (2010 - 1945))).cast(\"int\"))\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "# 6 - Filtrando as pessoas que nasceram neste século (após o ano 2000)\n",
    "# df_select = df_nomes.filter(df_nomes.AnoNascimento >= 2000)\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "# 9 - Selecionar as pessoas da geração Millennials utilizando Spark SQL\n",
    "\n",
    "# Registrar o DataFrame como uma tabela temporária\n",
    "df_nomes.createOrReplaceTempView(\"pessoas\")\n",
    "\n",
    "# Executar consulta SQL para contar pessoas da geração Millennials\n",
    "consulta_sql_millennials = \"\"\"\n",
    "    SELECT COUNT(*) AS NumeroMillennials\n",
    "    FROM pessoas\n",
    "    WHERE AnoNascimento BETWEEN 1980 AND 1994\n",
    "\"\"\"\n",
    "\n",
    "# Criar um novo DataFrame com os resultados da consulta SQL\n",
    "df_resultado_millennials = spark.sql(consulta_sql_millennials)\n",
    "\n",
    "# Mostra o resultado como uma tabela\n",
    "df_resultado_millennials.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 - Usando Spark SQL, obtenha a quantidade de pessoas de cada país para uma das gerações abaixo.\n",
    "Armazene o resultado em um novo dataframe e depois mostre todas as linhas em ordem crescente de Pais, Geração e Quantidade\n",
    "\n",
    "- Baby Boomers – nascidos entre 1944 e 1964;\n",
    "- Geração X – nascidos entre 1965 e 1979;4\n",
    "- Millennials (Geração Y) – nascidos entre 1980 e 1994;\n",
    "- Geração Z – nascidos entre 1995 e 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+----------+\n",
      "|     Pais|     Geracao|Quantidade|\n",
      "+---------+------------+----------+\n",
      "|Argentina|Baby Boomers|    436864|\n",
      "|Argentina|   Geração X|    328117|\n",
      "|Argentina|   Geração Z|    327884|\n",
      "|Argentina| Millennials|    328477|\n",
      "|  Bolívia|Baby Boomers|    120298|\n",
      "|  Bolívia|   Geração X|     89948|\n",
      "|  Bolívia|   Geração Z|     90121|\n",
      "|  Bolívia| Millennials|     90157|\n",
      "|   Brasil|Baby Boomers|    237248|\n",
      "|   Brasil|   Geração X|    177145|\n",
      "|   Brasil|   Geração Z|    177853|\n",
      "|   Brasil| Millennials|    177758|\n",
      "|    Chile|Baby Boomers|    556070|\n",
      "|    Chile|   Geração X|    415490|\n",
      "|    Chile|   Geração Z|    416266|\n",
      "|    Chile| Millennials|    416593|\n",
      "| Colômbia|Baby Boomers|    568144|\n",
      "| Colômbia|   Geração X|    426627|\n",
      "| Colômbia|   Geração Z|    427443|\n",
      "| Colômbia| Millennials|    427021|\n",
      "+---------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import rand, lit\n",
    "\n",
    "\n",
    "# Criando a Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Exercicio Intro\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Lendo o arquivo nomes_aleatorios.txt e carregando-o em um DataFrame\n",
    "caminho_arquivo = \"../Gerar_Massa_de_Dados/nomes_aleatorios.txt\"\n",
    "df_nomes = spark.read.text(caminho_arquivo)\n",
    "\n",
    "# 1- Lendo o arquivo nomes_aleatorios.txt e carregando-o em um DataFrame\n",
    "caminho_arquivo = \"../Gerar_Massa_de_Dados/nomes_aleatorios.txt\"\n",
    "df_nomes = spark.read.text(caminho_arquivo)\n",
    "#--------------------------------------------------------------------------------------\n",
    "# 2- Renomeando a coluna para 'Nomes'\n",
    "df_nomes = df_nomes.withColumnRenamed(\"value\", \"Nomes\")\n",
    "#--------------------------------------------------------------------------------------\n",
    "# 3 - Adicionando a nova coluna 'Escolaridade' com valores aleatórios\n",
    "df_nomes = df_nomes.withColumn('Escolaridade', when(rand() < 0.33, 'Fundamental')\n",
    "                                          .when(rand() < 0.66, 'Médio')\n",
    "                                          .otherwise('Superior'))\n",
    "#--------------------------------------------------------------------------------------\n",
    "# 4 - Lista de países da América do Sul\n",
    "paises_am_sul = [\"Brasil\", \"Argentina\", \"Chile\", \"Colômbia\", \"Equador\", \"Paraguai\", \"Uruguai\", \"Bolívia\", \"Peru\", \"Venezuela\", \"Suriname\", \"Guiana\", \"Guiana Francesa\"]\n",
    "                       \n",
    "# Adicionando a nova coluna com valores aleatórios\n",
    "df_nomes = df_nomes.withColumn(\"Pais\", \n",
    "    when(rand() < 0.077, paises_am_sul[0])\n",
    "    .when(rand() < 0.154, paises_am_sul[1])\n",
    "    .when(rand() < 0.231, paises_am_sul[2])\n",
    "    .when(rand() < 0.308, paises_am_sul[3])\n",
    "    .when(rand() < 0.385, paises_am_sul[4])\n",
    "    .when(rand() < 0.462, paises_am_sul[5])\n",
    "    .when(rand() < 0.539, paises_am_sul[6])\n",
    "    .when(rand() < 0.616, paises_am_sul[7])\n",
    "    .when(rand() < 0.693, paises_am_sul[8])\n",
    "    .when(rand() < 0.770, paises_am_sul[9])\n",
    "    .when(rand() < 0.847, paises_am_sul[10])\n",
    "    .when(rand() < 0.924, paises_am_sul[11])\n",
    "    .otherwise(paises_am_sul[12]))\n",
    "\n",
    "# 10 - Selecionar as pessoas da geração Millennials utilizando Spark SQL\n",
    "\n",
    "# Registrar o DataFrame como uma tabela temporária\n",
    "df_nomes.createOrReplaceTempView(\"pessoas\")\n",
    "\n",
    "# Executar consultas SQL para contar pessoas de cada geração por país\n",
    "consulta_sql_boomers = \"\"\"\n",
    "    SELECT Pais, 'Baby Boomers' AS Geracao, COUNT(*) AS Quantidade\n",
    "    FROM pessoas\n",
    "    WHERE AnoNascimento BETWEEN 1944 AND 1964\n",
    "    GROUP BY Pais\n",
    "\"\"\"\n",
    "\n",
    "consulta_sql_gen_x = \"\"\"\n",
    "    SELECT Pais, 'Geração X' AS Geracao, COUNT(*) AS Quantidade\n",
    "    FROM pessoas\n",
    "    WHERE AnoNascimento BETWEEN 1965 AND 1979\n",
    "    GROUP BY Pais\n",
    "\"\"\"\n",
    "\n",
    "consulta_sql_millennials = \"\"\"\n",
    "    SELECT Pais, 'Millennials' AS Geracao, COUNT(*) AS Quantidade\n",
    "    FROM pessoas\n",
    "    WHERE AnoNascimento BETWEEN 1980 AND 1994\n",
    "    GROUP BY Pais\n",
    "\"\"\"\n",
    "\n",
    "consulta_sql_gen_z = \"\"\"\n",
    "    SELECT Pais, 'Geração Z' AS Geracao, COUNT(*) AS Quantidade\n",
    "    FROM pessoas\n",
    "    WHERE AnoNascimento BETWEEN 1995 AND 2015\n",
    "    GROUP BY Pais\n",
    "\"\"\"\n",
    "\n",
    "# Unir os resultados das consultas em um único DataFrame\n",
    "df_resultado = spark.sql(consulta_sql_boomers).union(\n",
    "    spark.sql(consulta_sql_gen_x)).union(\n",
    "    spark.sql(consulta_sql_millennials)).union(\n",
    "    spark.sql(consulta_sql_gen_z))\n",
    "\n",
    "# Mostrar todas as linhas em ordem crescente de Pais, Geração e Quantidade\n",
    "df_resultado.orderBy(\"Pais\", \"Geracao\", \"Quantidade\").show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
